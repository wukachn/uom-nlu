{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Inference using BiLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data is in the form: premise, hypothesis, label\n",
    "### with label being either 1 (entailment), 0 (neutral, or contradiction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings...\n",
      "Loaded 400001 word vectors.\n"
     ]
    }
   ],
   "source": [
    "def load_glove_embeddings(embedding_path):\n",
    "    print(\"Loading GloVe embeddings...\")\n",
    "    embeddings_index = {}\n",
    "    with open(embedding_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    print(f\"Loaded {len(embeddings_index)} word vectors.\")\n",
    "    return embeddings_index\n",
    "\n",
    "def sentence_embedding(sentence, embeddings_index):\n",
    "    words = sentence.split()\n",
    "    embedding_dim = next(iter(embeddings_index.values())).shape[0]\n",
    "    sentence_embedding = np.zeros(embedding_dim)\n",
    "    for word in words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            sentence_embedding += embedding_vector\n",
    "    return sentence_embedding / len(words)\n",
    "\n",
    "embedding_path = \"./input/embeddings/glove.6B/glove.6B.300d.txt\"\n",
    "embeddings_index = load_glove_embeddings(embedding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/train.csv\"\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "premise_embeddings = [sentence_embedding(sentence.lower(), embeddings_index) for sentence in df['premise']]\n",
    "hypothesis_embeddings = [sentence_embedding(sentence.lower(), embeddings_index) for sentence in df['hypothesis']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((np.array(premise_embeddings), np.array(hypothesis_embeddings)))\n",
    "y = df['label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5114 - loss: 0.9614 - val_accuracy: 0.5144 - val_loss: 0.8313\n",
      "Epoch 2/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5266 - loss: 0.8066 - val_accuracy: 0.5278 - val_loss: 0.7585\n",
      "Epoch 3/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5296 - loss: 0.7510 - val_accuracy: 0.5371 - val_loss: 0.7292\n",
      "Epoch 4/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5299 - loss: 0.7272 - val_accuracy: 0.5431 - val_loss: 0.7149\n",
      "Epoch 5/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5410 - loss: 0.7131 - val_accuracy: 0.5436 - val_loss: 0.7069\n",
      "Epoch 6/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5422 - loss: 0.7073 - val_accuracy: 0.5515 - val_loss: 0.7013\n",
      "Epoch 7/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5498 - loss: 0.7014 - val_accuracy: 0.5557 - val_loss: 0.6972\n",
      "Epoch 8/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5621 - loss: 0.6961 - val_accuracy: 0.5571 - val_loss: 0.6939\n",
      "Epoch 9/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5639 - loss: 0.6933 - val_accuracy: 0.5622 - val_loss: 0.6913\n",
      "Epoch 10/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5664 - loss: 0.6908 - val_accuracy: 0.5659 - val_loss: 0.6897\n",
      "Epoch 11/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5710 - loss: 0.6887 - val_accuracy: 0.5733 - val_loss: 0.6877\n",
      "Epoch 12/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5750 - loss: 0.6875 - val_accuracy: 0.5821 - val_loss: 0.6859\n",
      "Epoch 13/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5771 - loss: 0.6863 - val_accuracy: 0.5840 - val_loss: 0.6847\n",
      "Epoch 14/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5814 - loss: 0.6838 - val_accuracy: 0.5904 - val_loss: 0.6831\n",
      "Epoch 15/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5907 - loss: 0.6815 - val_accuracy: 0.5877 - val_loss: 0.6821\n",
      "Epoch 16/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5867 - loss: 0.6812 - val_accuracy: 0.5909 - val_loss: 0.6803\n",
      "Epoch 17/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5875 - loss: 0.6808 - val_accuracy: 0.5974 - val_loss: 0.6793\n",
      "Epoch 18/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5903 - loss: 0.6793 - val_accuracy: 0.5955 - val_loss: 0.6785\n",
      "Epoch 19/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5963 - loss: 0.6780 - val_accuracy: 0.5979 - val_loss: 0.6773\n",
      "Epoch 20/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5995 - loss: 0.6754 - val_accuracy: 0.6011 - val_loss: 0.6762\n",
      "Epoch 21/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5960 - loss: 0.6767 - val_accuracy: 0.6011 - val_loss: 0.6755\n",
      "Epoch 22/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5936 - loss: 0.6759 - val_accuracy: 0.6053 - val_loss: 0.6749\n",
      "Epoch 23/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5997 - loss: 0.6745 - val_accuracy: 0.6062 - val_loss: 0.6739\n",
      "Epoch 24/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6062 - loss: 0.6721 - val_accuracy: 0.6076 - val_loss: 0.6730\n",
      "Epoch 25/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6108 - loss: 0.6700 - val_accuracy: 0.6030 - val_loss: 0.6717\n",
      "Epoch 26/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6004 - loss: 0.6722 - val_accuracy: 0.6081 - val_loss: 0.6715\n",
      "Epoch 27/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6053 - loss: 0.6704 - val_accuracy: 0.6067 - val_loss: 0.6703\n",
      "Epoch 28/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6128 - loss: 0.6681 - val_accuracy: 0.6071 - val_loss: 0.6695\n",
      "Epoch 29/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6060 - loss: 0.6702 - val_accuracy: 0.6076 - val_loss: 0.6689\n",
      "Epoch 30/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6126 - loss: 0.6676 - val_accuracy: 0.6071 - val_loss: 0.6684\n",
      "Epoch 31/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6155 - loss: 0.6653 - val_accuracy: 0.6081 - val_loss: 0.6674\n",
      "Epoch 32/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6188 - loss: 0.6650 - val_accuracy: 0.6081 - val_loss: 0.6669\n",
      "Epoch 33/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6124 - loss: 0.6655 - val_accuracy: 0.6076 - val_loss: 0.6661\n",
      "Epoch 34/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6128 - loss: 0.6659 - val_accuracy: 0.6076 - val_loss: 0.6650\n",
      "Epoch 35/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6123 - loss: 0.6644 - val_accuracy: 0.6076 - val_loss: 0.6649\n",
      "Epoch 36/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6184 - loss: 0.6639 - val_accuracy: 0.6099 - val_loss: 0.6640\n",
      "Epoch 37/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6166 - loss: 0.6617 - val_accuracy: 0.6099 - val_loss: 0.6634\n",
      "Epoch 38/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6142 - loss: 0.6635 - val_accuracy: 0.6090 - val_loss: 0.6631\n",
      "Epoch 39/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6170 - loss: 0.6620 - val_accuracy: 0.6109 - val_loss: 0.6622\n",
      "Epoch 40/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6155 - loss: 0.6611 - val_accuracy: 0.6095 - val_loss: 0.6617\n",
      "Epoch 41/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6126 - loss: 0.6619 - val_accuracy: 0.6109 - val_loss: 0.6611\n",
      "Epoch 42/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6211 - loss: 0.6589 - val_accuracy: 0.6104 - val_loss: 0.6606\n",
      "Epoch 43/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6199 - loss: 0.6584 - val_accuracy: 0.6141 - val_loss: 0.6603\n",
      "Epoch 44/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6253 - loss: 0.6591 - val_accuracy: 0.6127 - val_loss: 0.6594\n",
      "Epoch 45/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6210 - loss: 0.6581 - val_accuracy: 0.6132 - val_loss: 0.6588\n",
      "Epoch 46/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6226 - loss: 0.6568 - val_accuracy: 0.6146 - val_loss: 0.6586\n",
      "Epoch 47/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6233 - loss: 0.6560 - val_accuracy: 0.6122 - val_loss: 0.6578\n",
      "Epoch 48/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6181 - loss: 0.6584 - val_accuracy: 0.6127 - val_loss: 0.6573\n",
      "Epoch 49/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6228 - loss: 0.6576 - val_accuracy: 0.6141 - val_loss: 0.6571\n",
      "Epoch 50/50\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6242 - loss: 0.6548 - val_accuracy: 0.6127 - val_loss: 0.6563\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397us/step - accuracy: 0.6285 - loss: 0.6547\n",
      "Test Loss: 0.6564717292785645\n",
      "Test Accuracy: 0.6233067512512207\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(600,)),  # input shape is twice the GloVe embedding dimension for premise and hypothesis\n",
    "    tf.keras.layers.Dense(200, activation='tanh'),\n",
    "    tf.keras.layers.Dense(200, activation='tanh'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')  # 3 classes: entailment, contradiction, neutral\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adadelta',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                premise hypothesis  label\n",
      "3126  Tony  Shoes (so Clinton will have Shoes and So...        NaN      1\n",
      "3970                            Saint-Germain-des-Pr??s        NaN      1\n",
      "Empty DataFrame\n",
      "Columns: [premise, hypothesis, label]\n",
      "Index: []\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420us/step - accuracy: 0.6076 - loss: 0.6611\n",
      "Dev Loss: 0.6605627536773682\n",
      "Dev Accuracy: 0.6103934645652771\n"
     ]
    }
   ],
   "source": [
    "dev_data_path = \"./data/dev.csv\"\n",
    "dev_df = pd.read_csv(dev_data_path)\n",
    "\n",
    "# Print all rows in the dev dataframe where there is a NaN value\n",
    "print(dev_df[dev_df.isna().any(axis=1)])\n",
    "\n",
    "# Remove those rows\n",
    "dev_df = dev_df.dropna()\n",
    "\n",
    "print(dev_df[dev_df.isna().any(axis=1)])\n",
    "\n",
    "# Test the model on the dev set\n",
    "premise_embeddings = [sentence_embedding(sentence.lower(), embeddings_index) for sentence in dev_df['premise']]\n",
    "hypothesis_embeddings = [sentence_embedding(sentence.lower(), embeddings_index) for sentence in dev_df['hypothesis']]\n",
    "X_dev = np.hstack((np.array(premise_embeddings), np.array(hypothesis_embeddings)))\n",
    "y_dev = dev_df['label'].values\n",
    "\n",
    "loss, accuracy = model.evaluate(X_dev, y_dev)\n",
    "print(f\"Dev Loss: {loss}\")\n",
    "print(f\"Dev Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
