{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Inference using BiLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data is in the form: premise, hypothesis, label\n",
    "### with label being either 1 (entailment), 0 (neutral, or contradiction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings...\n",
      "Loaded 400001 word vectors.\n"
     ]
    }
   ],
   "source": [
    "def load_glove_embeddings(embedding_path):\n",
    "    print(\"Loading GloVe embeddings...\")\n",
    "    embeddings_index = {}\n",
    "    with open(embedding_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    print(f\"Loaded {len(embeddings_index)} word vectors.\")\n",
    "    return embeddings_index\n",
    "\n",
    "def sentence_embedding(sentence, embeddings_index):\n",
    "    words = sentence.split()\n",
    "    embedding_dim = next(iter(embeddings_index.values())).shape[0]\n",
    "    sentence_embedding = np.zeros(embedding_dim)\n",
    "    for word in words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            sentence_embedding += embedding_vector\n",
    "    return sentence_embedding\n",
    "\n",
    "embedding_path = \"./input/embeddings/glove.6B/glove.6B.300d.txt\"\n",
    "embeddings_index = load_glove_embeddings(embedding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/train.csv\"\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "premise_embeddings = [sentence_embedding(sentence.lower(), embeddings_index) for sentence in df['premise']]\n",
    "hypothesis_embeddings = [sentence_embedding(sentence.lower(), embeddings_index) for sentence in df['hypothesis']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((np.array(premise_embeddings), np.array(hypothesis_embeddings)))\n",
    "y = df['label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.4911 - loss: 0.7898 - val_accuracy: 0.4884 - val_loss: 0.7125\n",
      "Epoch 2/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5128 - loss: 0.7519 - val_accuracy: 0.5028 - val_loss: 0.7044\n",
      "Epoch 3/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5119 - loss: 0.7537 - val_accuracy: 0.5241 - val_loss: 0.6985\n",
      "Epoch 4/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5093 - loss: 0.7495 - val_accuracy: 0.5311 - val_loss: 0.6950\n",
      "Epoch 5/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5223 - loss: 0.7410 - val_accuracy: 0.5343 - val_loss: 0.6915\n",
      "Epoch 6/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5159 - loss: 0.7377 - val_accuracy: 0.5357 - val_loss: 0.6899\n",
      "Epoch 7/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5227 - loss: 0.7330 - val_accuracy: 0.5417 - val_loss: 0.6873\n",
      "Epoch 8/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5297 - loss: 0.7245 - val_accuracy: 0.5450 - val_loss: 0.6854\n",
      "Epoch 9/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5262 - loss: 0.7300 - val_accuracy: 0.5468 - val_loss: 0.6845\n",
      "Epoch 10/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5251 - loss: 0.7224 - val_accuracy: 0.5519 - val_loss: 0.6825\n",
      "Epoch 11/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5265 - loss: 0.7265 - val_accuracy: 0.5538 - val_loss: 0.6813\n",
      "Epoch 12/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5326 - loss: 0.7164 - val_accuracy: 0.5552 - val_loss: 0.6806\n",
      "Epoch 13/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5243 - loss: 0.7224 - val_accuracy: 0.5584 - val_loss: 0.6794\n",
      "Epoch 14/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5324 - loss: 0.7153 - val_accuracy: 0.5580 - val_loss: 0.6786\n",
      "Epoch 15/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5349 - loss: 0.7141 - val_accuracy: 0.5612 - val_loss: 0.6781\n",
      "Epoch 16/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5427 - loss: 0.7074 - val_accuracy: 0.5659 - val_loss: 0.6768\n",
      "Epoch 17/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5410 - loss: 0.7115 - val_accuracy: 0.5659 - val_loss: 0.6769\n",
      "Epoch 18/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5387 - loss: 0.7085 - val_accuracy: 0.5696 - val_loss: 0.6761\n",
      "Epoch 19/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5411 - loss: 0.7093 - val_accuracy: 0.5705 - val_loss: 0.6750\n",
      "Epoch 20/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5382 - loss: 0.7085 - val_accuracy: 0.5737 - val_loss: 0.6739\n",
      "Epoch 21/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5396 - loss: 0.7069 - val_accuracy: 0.5770 - val_loss: 0.6737\n",
      "Epoch 22/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5436 - loss: 0.7052 - val_accuracy: 0.5775 - val_loss: 0.6731\n",
      "Epoch 23/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5422 - loss: 0.7035 - val_accuracy: 0.5793 - val_loss: 0.6731\n",
      "Epoch 24/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5434 - loss: 0.7065 - val_accuracy: 0.5816 - val_loss: 0.6722\n",
      "Epoch 25/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5464 - loss: 0.6989 - val_accuracy: 0.5821 - val_loss: 0.6714\n",
      "Epoch 26/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5431 - loss: 0.6995 - val_accuracy: 0.5826 - val_loss: 0.6710\n",
      "Epoch 27/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5501 - loss: 0.7001 - val_accuracy: 0.5835 - val_loss: 0.6702\n",
      "Epoch 28/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5475 - loss: 0.6989 - val_accuracy: 0.5835 - val_loss: 0.6693\n",
      "Epoch 29/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5448 - loss: 0.6992 - val_accuracy: 0.5858 - val_loss: 0.6691\n",
      "Epoch 30/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5559 - loss: 0.6942 - val_accuracy: 0.5863 - val_loss: 0.6682\n",
      "Epoch 31/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5487 - loss: 0.6979 - val_accuracy: 0.5891 - val_loss: 0.6681\n",
      "Epoch 32/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5525 - loss: 0.6971 - val_accuracy: 0.5900 - val_loss: 0.6674\n",
      "Epoch 33/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5553 - loss: 0.6924 - val_accuracy: 0.5904 - val_loss: 0.6673\n",
      "Epoch 34/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5572 - loss: 0.6921 - val_accuracy: 0.5904 - val_loss: 0.6668\n",
      "Epoch 35/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5524 - loss: 0.6921 - val_accuracy: 0.5955 - val_loss: 0.6658\n",
      "Epoch 36/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5594 - loss: 0.6918 - val_accuracy: 0.5979 - val_loss: 0.6651\n",
      "Epoch 37/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5502 - loss: 0.6945 - val_accuracy: 0.5974 - val_loss: 0.6653\n",
      "Epoch 38/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5680 - loss: 0.6848 - val_accuracy: 0.6002 - val_loss: 0.6644\n",
      "Epoch 39/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5584 - loss: 0.6905 - val_accuracy: 0.5997 - val_loss: 0.6635\n",
      "Epoch 40/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5639 - loss: 0.6878 - val_accuracy: 0.5988 - val_loss: 0.6632\n",
      "Epoch 41/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5588 - loss: 0.6862 - val_accuracy: 0.5993 - val_loss: 0.6631\n",
      "Epoch 42/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5632 - loss: 0.6888 - val_accuracy: 0.5993 - val_loss: 0.6635\n",
      "Epoch 43/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5646 - loss: 0.6856 - val_accuracy: 0.5993 - val_loss: 0.6616\n",
      "Epoch 44/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5624 - loss: 0.6851 - val_accuracy: 0.5997 - val_loss: 0.6611\n",
      "Epoch 45/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5725 - loss: 0.6848 - val_accuracy: 0.5993 - val_loss: 0.6617\n",
      "Epoch 46/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5735 - loss: 0.6823 - val_accuracy: 0.6002 - val_loss: 0.6608\n",
      "Epoch 47/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5691 - loss: 0.6827 - val_accuracy: 0.6011 - val_loss: 0.6604\n",
      "Epoch 48/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5710 - loss: 0.6853 - val_accuracy: 0.6002 - val_loss: 0.6600\n",
      "Epoch 49/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5761 - loss: 0.6754 - val_accuracy: 0.6039 - val_loss: 0.6588\n",
      "Epoch 50/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5757 - loss: 0.6781 - val_accuracy: 0.6025 - val_loss: 0.6587\n",
      "Epoch 51/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5713 - loss: 0.6806 - val_accuracy: 0.6025 - val_loss: 0.6582\n",
      "Epoch 52/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5805 - loss: 0.6776 - val_accuracy: 0.6071 - val_loss: 0.6574\n",
      "Epoch 53/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5751 - loss: 0.6744 - val_accuracy: 0.6053 - val_loss: 0.6578\n",
      "Epoch 54/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5700 - loss: 0.6821 - val_accuracy: 0.6099 - val_loss: 0.6562\n",
      "Epoch 55/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5776 - loss: 0.6800 - val_accuracy: 0.6076 - val_loss: 0.6565\n",
      "Epoch 56/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5781 - loss: 0.6736 - val_accuracy: 0.6090 - val_loss: 0.6561\n",
      "Epoch 57/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5736 - loss: 0.6804 - val_accuracy: 0.6132 - val_loss: 0.6552\n",
      "Epoch 58/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5777 - loss: 0.6751 - val_accuracy: 0.6109 - val_loss: 0.6553\n",
      "Epoch 59/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5880 - loss: 0.6681 - val_accuracy: 0.6104 - val_loss: 0.6545\n",
      "Epoch 60/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5825 - loss: 0.6742 - val_accuracy: 0.6141 - val_loss: 0.6542\n",
      "Epoch 61/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5806 - loss: 0.6774 - val_accuracy: 0.6169 - val_loss: 0.6541\n",
      "Epoch 62/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5780 - loss: 0.6776 - val_accuracy: 0.6155 - val_loss: 0.6537\n",
      "Epoch 63/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5840 - loss: 0.6699 - val_accuracy: 0.6136 - val_loss: 0.6530\n",
      "Epoch 64/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5888 - loss: 0.6697 - val_accuracy: 0.6136 - val_loss: 0.6530\n",
      "Epoch 65/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5970 - loss: 0.6704 - val_accuracy: 0.6146 - val_loss: 0.6522\n",
      "Epoch 66/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5830 - loss: 0.6734 - val_accuracy: 0.6164 - val_loss: 0.6525\n",
      "Epoch 67/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5842 - loss: 0.6701 - val_accuracy: 0.6169 - val_loss: 0.6520\n",
      "Epoch 68/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5855 - loss: 0.6689 - val_accuracy: 0.6169 - val_loss: 0.6522\n",
      "Epoch 69/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5992 - loss: 0.6645 - val_accuracy: 0.6164 - val_loss: 0.6510\n",
      "Epoch 70/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5816 - loss: 0.6702 - val_accuracy: 0.6187 - val_loss: 0.6503\n",
      "Epoch 71/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5829 - loss: 0.6699 - val_accuracy: 0.6178 - val_loss: 0.6501\n",
      "Epoch 72/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5932 - loss: 0.6667 - val_accuracy: 0.6215 - val_loss: 0.6510\n",
      "Epoch 73/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5914 - loss: 0.6669 - val_accuracy: 0.6206 - val_loss: 0.6499\n",
      "Epoch 74/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5930 - loss: 0.6657 - val_accuracy: 0.6183 - val_loss: 0.6498\n",
      "Epoch 75/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5975 - loss: 0.6625 - val_accuracy: 0.6248 - val_loss: 0.6491\n",
      "Epoch 76/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5923 - loss: 0.6673 - val_accuracy: 0.6183 - val_loss: 0.6502\n",
      "Epoch 77/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5947 - loss: 0.6687 - val_accuracy: 0.6238 - val_loss: 0.6489\n",
      "Epoch 78/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6025 - loss: 0.6600 - val_accuracy: 0.6234 - val_loss: 0.6484\n",
      "Epoch 79/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5977 - loss: 0.6629 - val_accuracy: 0.6238 - val_loss: 0.6484\n",
      "Epoch 80/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5988 - loss: 0.6633 - val_accuracy: 0.6234 - val_loss: 0.6483\n",
      "Epoch 81/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6006 - loss: 0.6595 - val_accuracy: 0.6276 - val_loss: 0.6476\n",
      "Epoch 82/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6007 - loss: 0.6644 - val_accuracy: 0.6234 - val_loss: 0.6481\n",
      "Epoch 83/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5990 - loss: 0.6618 - val_accuracy: 0.6262 - val_loss: 0.6467\n",
      "Epoch 84/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6012 - loss: 0.6651 - val_accuracy: 0.6257 - val_loss: 0.6472\n",
      "Epoch 85/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6047 - loss: 0.6566 - val_accuracy: 0.6229 - val_loss: 0.6472\n",
      "Epoch 86/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6038 - loss: 0.6598 - val_accuracy: 0.6220 - val_loss: 0.6469\n",
      "Epoch 87/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6023 - loss: 0.6593 - val_accuracy: 0.6257 - val_loss: 0.6460\n",
      "Epoch 88/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6074 - loss: 0.6595 - val_accuracy: 0.6220 - val_loss: 0.6468\n",
      "Epoch 89/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6072 - loss: 0.6549 - val_accuracy: 0.6262 - val_loss: 0.6459\n",
      "Epoch 90/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6052 - loss: 0.6574 - val_accuracy: 0.6229 - val_loss: 0.6466\n",
      "Epoch 91/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6080 - loss: 0.6549 - val_accuracy: 0.6262 - val_loss: 0.6456\n",
      "Epoch 92/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6111 - loss: 0.6520 - val_accuracy: 0.6248 - val_loss: 0.6451\n",
      "Epoch 93/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6071 - loss: 0.6567 - val_accuracy: 0.6262 - val_loss: 0.6453\n",
      "Epoch 94/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6100 - loss: 0.6545 - val_accuracy: 0.6252 - val_loss: 0.6449\n",
      "Epoch 95/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6138 - loss: 0.6508 - val_accuracy: 0.6248 - val_loss: 0.6452\n",
      "Epoch 96/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6066 - loss: 0.6560 - val_accuracy: 0.6238 - val_loss: 0.6454\n",
      "Epoch 97/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6101 - loss: 0.6542 - val_accuracy: 0.6243 - val_loss: 0.6444\n",
      "Epoch 98/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6152 - loss: 0.6498 - val_accuracy: 0.6238 - val_loss: 0.6449\n",
      "Epoch 99/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6117 - loss: 0.6536 - val_accuracy: 0.6234 - val_loss: 0.6444\n",
      "Epoch 100/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6206 - loss: 0.6528 - val_accuracy: 0.6224 - val_loss: 0.6449\n",
      "Epoch 101/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6085 - loss: 0.6516 - val_accuracy: 0.6234 - val_loss: 0.6444\n",
      "Epoch 102/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6073 - loss: 0.6550 - val_accuracy: 0.6252 - val_loss: 0.6442\n",
      "Epoch 103/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6196 - loss: 0.6519 - val_accuracy: 0.6266 - val_loss: 0.6441\n",
      "Epoch 104/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6199 - loss: 0.6481 - val_accuracy: 0.6276 - val_loss: 0.6437\n",
      "Epoch 105/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6158 - loss: 0.6493 - val_accuracy: 0.6243 - val_loss: 0.6443\n",
      "Epoch 106/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6117 - loss: 0.6508 - val_accuracy: 0.6262 - val_loss: 0.6437\n",
      "Epoch 107/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6180 - loss: 0.6502 - val_accuracy: 0.6271 - val_loss: 0.6438\n",
      "Epoch 108/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6206 - loss: 0.6460 - val_accuracy: 0.6285 - val_loss: 0.6428\n",
      "Epoch 109/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6192 - loss: 0.6468 - val_accuracy: 0.6276 - val_loss: 0.6429\n",
      "Epoch 110/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6219 - loss: 0.6497 - val_accuracy: 0.6289 - val_loss: 0.6436\n",
      "Epoch 111/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6164 - loss: 0.6479 - val_accuracy: 0.6289 - val_loss: 0.6436\n",
      "Epoch 112/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6155 - loss: 0.6457 - val_accuracy: 0.6271 - val_loss: 0.6431\n",
      "Epoch 113/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6192 - loss: 0.6484 - val_accuracy: 0.6280 - val_loss: 0.6431\n",
      "Epoch 114/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6191 - loss: 0.6457 - val_accuracy: 0.6248 - val_loss: 0.6419\n",
      "Epoch 115/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6232 - loss: 0.6466 - val_accuracy: 0.6243 - val_loss: 0.6421\n",
      "Epoch 116/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6264 - loss: 0.6448 - val_accuracy: 0.6289 - val_loss: 0.6434\n",
      "Epoch 117/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6225 - loss: 0.6446 - val_accuracy: 0.6257 - val_loss: 0.6422\n",
      "Epoch 118/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6313 - loss: 0.6393 - val_accuracy: 0.6257 - val_loss: 0.6425\n",
      "Epoch 119/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6248 - loss: 0.6392 - val_accuracy: 0.6271 - val_loss: 0.6421\n",
      "Epoch 120/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6241 - loss: 0.6424 - val_accuracy: 0.6262 - val_loss: 0.6424\n",
      "Epoch 121/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6315 - loss: 0.6394 - val_accuracy: 0.6271 - val_loss: 0.6428\n",
      "Epoch 122/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6239 - loss: 0.6442 - val_accuracy: 0.6271 - val_loss: 0.6427\n",
      "Epoch 123/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6261 - loss: 0.6393 - val_accuracy: 0.6276 - val_loss: 0.6420\n",
      "Epoch 124/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6311 - loss: 0.6419 - val_accuracy: 0.6289 - val_loss: 0.6421\n",
      "Epoch 125/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6328 - loss: 0.6356 - val_accuracy: 0.6313 - val_loss: 0.6428\n",
      "Epoch 126/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6185 - loss: 0.6449 - val_accuracy: 0.6276 - val_loss: 0.6422\n",
      "Epoch 127/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6253 - loss: 0.6427 - val_accuracy: 0.6303 - val_loss: 0.6424\n",
      "Epoch 128/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6230 - loss: 0.6397 - val_accuracy: 0.6313 - val_loss: 0.6420\n",
      "Epoch 129/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6286 - loss: 0.6391 - val_accuracy: 0.6285 - val_loss: 0.6412\n",
      "Epoch 130/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6330 - loss: 0.6373 - val_accuracy: 0.6289 - val_loss: 0.6427\n",
      "Epoch 131/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6351 - loss: 0.6367 - val_accuracy: 0.6327 - val_loss: 0.6417\n",
      "Epoch 132/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6373 - loss: 0.6376 - val_accuracy: 0.6322 - val_loss: 0.6417\n",
      "Epoch 133/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6343 - loss: 0.6363 - val_accuracy: 0.6336 - val_loss: 0.6416\n",
      "Epoch 134/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6277 - loss: 0.6391 - val_accuracy: 0.6299 - val_loss: 0.6424\n",
      "Epoch 135/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6344 - loss: 0.6338 - val_accuracy: 0.6331 - val_loss: 0.6420\n",
      "Epoch 136/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6343 - loss: 0.6378 - val_accuracy: 0.6308 - val_loss: 0.6421\n",
      "Epoch 137/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6349 - loss: 0.6393 - val_accuracy: 0.6336 - val_loss: 0.6419\n",
      "Epoch 138/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6293 - loss: 0.6371 - val_accuracy: 0.6285 - val_loss: 0.6428\n",
      "Epoch 139/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6381 - loss: 0.6345 - val_accuracy: 0.6308 - val_loss: 0.6423\n",
      "Epoch 140/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6373 - loss: 0.6331 - val_accuracy: 0.6317 - val_loss: 0.6421\n",
      "Epoch 141/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6343 - loss: 0.6352 - val_accuracy: 0.6322 - val_loss: 0.6419\n",
      "Epoch 142/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6402 - loss: 0.6294 - val_accuracy: 0.6322 - val_loss: 0.6421\n",
      "Epoch 143/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6423 - loss: 0.6312 - val_accuracy: 0.6299 - val_loss: 0.6416\n",
      "Epoch 144/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6324 - loss: 0.6363 - val_accuracy: 0.6289 - val_loss: 0.6416\n",
      "Epoch 145/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6397 - loss: 0.6323 - val_accuracy: 0.6327 - val_loss: 0.6419\n",
      "Epoch 146/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6432 - loss: 0.6295 - val_accuracy: 0.6317 - val_loss: 0.6417\n",
      "Epoch 147/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6365 - loss: 0.6319 - val_accuracy: 0.6327 - val_loss: 0.6424\n",
      "Epoch 148/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6422 - loss: 0.6294 - val_accuracy: 0.6303 - val_loss: 0.6417\n",
      "Epoch 149/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6410 - loss: 0.6290 - val_accuracy: 0.6289 - val_loss: 0.6431\n",
      "Epoch 150/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6414 - loss: 0.6259 - val_accuracy: 0.6303 - val_loss: 0.6428\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - accuracy: 0.6226 - loss: 0.6487\n",
      "Test Loss: 0.6492131948471069\n",
      "Test Accuracy: 0.6270179748535156\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(600,)),  # input shape is twice the GloVe embedding dimension for premise and hypothesis\n",
    "    tf.keras.layers.Dense(200, activation='tanh'),\n",
    "    tf.keras.layers.Dense(200, activation='tanh'),\n",
    "    tf.keras.layers.Dense(200, activation='tanh'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(200, activation='tanh'),\n",
    "    tf.keras.layers.Dense(200, activation='tanh'),\n",
    "    tf.keras.layers.Dense(200, activation='tanh'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(200, activation='tanh'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')  # 2 classes: entailment, contradiction\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adadelta',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=150, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                premise hypothesis  label\n",
      "3126  Tony  Shoes (so Clinton will have Shoes and So...        NaN      1\n",
      "3970                            Saint-Germain-des-Pr??s        NaN      1\n",
      "Empty DataFrame\n",
      "Columns: [premise, hypothesis, label]\n",
      "Index: []\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 823us/step - accuracy: 0.6328 - loss: 0.6442\n",
      "Dev Loss: 0.6491600275039673\n",
      "Dev Accuracy: 0.6271337270736694\n"
     ]
    }
   ],
   "source": [
    "dev_data_path = \"./data/dev.csv\"\n",
    "dev_df = pd.read_csv(dev_data_path)\n",
    "\n",
    "# Print all rows in the dev dataframe where there is a NaN value\n",
    "print(dev_df[dev_df.isna().any(axis=1)])\n",
    "\n",
    "# Convert NaN values to empty strings\n",
    "dev_df = dev_df.fillna('')\n",
    "# Print all rows in the dev dataframe where there is a NaN value\n",
    "print(dev_df[dev_df.isna().any(axis=1)])\n",
    "\n",
    "# Test the model on the dev set\n",
    "premise_embeddings = [sentence_embedding(sentence.lower(), embeddings_index) for sentence in dev_df['premise']]\n",
    "hypothesis_embeddings = [sentence_embedding(sentence.lower(), embeddings_index) for sentence in dev_df['hypothesis']]\n",
    "X_dev = np.hstack((np.array(premise_embeddings), np.array(hypothesis_embeddings)))\n",
    "y_dev = dev_df['label'].values\n",
    "\n",
    "loss, accuracy = model.evaluate(X_dev, y_dev)\n",
    "print(f\"Dev Loss: {loss}\")\n",
    "print(f\"Dev Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create sentence embeddings using plain RNN\n",
    "def sentence_embedding_rnn(sentence, embeddings_index, rnn_units=300):\n",
    "    words = sentence.split()\n",
    "    embedding_dim = next(iter(embeddings_index.values())).shape[0]\n",
    "    sentence_embedding = np.zeros(embedding_dim)\n",
    "    for word in words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            sentence_embedding += embedding_vector\n",
    "\n",
    "    # Create plain RNN layer\n",
    "    rnn_layer = tf.keras.layers.SimpleRNN(rnn_units)\n",
    "    \n",
    "    # Process sentence embedding through RNN\n",
    "    sentence_embedding_rnn = rnn_layer(tf.expand_dims([sentence_embedding], axis=0))\n",
    "    \n",
    "    return tf.squeeze(sentence_embedding_rnn, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings...\n",
      "Loaded 400001 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embedding_path = \"./input/embeddings/glove.6B/glove.6B.300d.txt\"\n",
    "embeddings_index = load_glove_embeddings(embedding_path)\n",
    "\n",
    "# Create sentence embeddings for premise and hypothesis using plain RNN\n",
    "premise_embeddings_rnn = [sentence_embedding_rnn(sentence.lower(), embeddings_index) for sentence in df['premise']]\n",
    "hypothesis_embeddings_rnn = [sentence_embedding_rnn(sentence.lower(), embeddings_index) for sentence in df['hypothesis']]\n",
    "\n",
    "# Combine premise and hypothesis embeddings\n",
    "X_rnn = np.hstack((np.array(premise_embeddings_rnn), np.array(hypothesis_embeddings_rnn)))\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train_rnn, X_test_rnn, y_train_rnn, y_test_rnn = train_test_split(X_rnn, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 23ms/step - accuracy: 0.5078 - loss: 0.7358 - val_accuracy: 0.5032 - val_loss: 0.7227\n",
      "Epoch 2/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5067 - loss: 0.7298 - val_accuracy: 0.4986 - val_loss: 0.7179\n",
      "Epoch 3/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4997 - loss: 0.7270 - val_accuracy: 0.4991 - val_loss: 0.7141\n",
      "Epoch 4/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5108 - loss: 0.7179 - val_accuracy: 0.4958 - val_loss: 0.7114\n",
      "Epoch 5/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5040 - loss: 0.7154 - val_accuracy: 0.4949 - val_loss: 0.7091\n",
      "Epoch 6/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4983 - loss: 0.7157 - val_accuracy: 0.4972 - val_loss: 0.7071\n",
      "Epoch 7/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5026 - loss: 0.7129 - val_accuracy: 0.4991 - val_loss: 0.7054\n",
      "Epoch 8/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5071 - loss: 0.7101 - val_accuracy: 0.4972 - val_loss: 0.7042\n",
      "Epoch 9/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5092 - loss: 0.7058 - val_accuracy: 0.4986 - val_loss: 0.7033\n",
      "Epoch 10/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5195 - loss: 0.7031 - val_accuracy: 0.4995 - val_loss: 0.7025\n",
      "Epoch 11/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5118 - loss: 0.7039 - val_accuracy: 0.4981 - val_loss: 0.7017\n",
      "Epoch 12/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5105 - loss: 0.7038 - val_accuracy: 0.4995 - val_loss: 0.7010\n",
      "Epoch 13/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5149 - loss: 0.7010 - val_accuracy: 0.4968 - val_loss: 0.7005\n",
      "Epoch 14/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5056 - loss: 0.7039 - val_accuracy: 0.4981 - val_loss: 0.7000\n",
      "Epoch 15/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5128 - loss: 0.7018 - val_accuracy: 0.5028 - val_loss: 0.6996\n",
      "Epoch 16/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.5155 - loss: 0.6982 - val_accuracy: 0.5032 - val_loss: 0.6993\n",
      "Epoch 17/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5155 - loss: 0.6997 - val_accuracy: 0.5009 - val_loss: 0.6991\n",
      "Epoch 18/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5104 - loss: 0.7000 - val_accuracy: 0.5000 - val_loss: 0.6988\n",
      "Epoch 19/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5159 - loss: 0.6992 - val_accuracy: 0.5019 - val_loss: 0.6985\n",
      "Epoch 20/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5104 - loss: 0.6983 - val_accuracy: 0.5037 - val_loss: 0.6984\n",
      "Epoch 21/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5184 - loss: 0.6966 - val_accuracy: 0.5046 - val_loss: 0.6982\n",
      "Epoch 22/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5127 - loss: 0.6979 - val_accuracy: 0.5051 - val_loss: 0.6979\n",
      "Epoch 23/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5168 - loss: 0.6962 - val_accuracy: 0.5074 - val_loss: 0.6978\n",
      "Epoch 24/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5140 - loss: 0.6971 - val_accuracy: 0.5093 - val_loss: 0.6977\n",
      "Epoch 25/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5118 - loss: 0.6985 - val_accuracy: 0.5116 - val_loss: 0.6976\n",
      "Epoch 26/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5234 - loss: 0.6951 - val_accuracy: 0.5116 - val_loss: 0.6975\n",
      "Epoch 27/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5193 - loss: 0.6960 - val_accuracy: 0.5093 - val_loss: 0.6973\n",
      "Epoch 28/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.5299 - loss: 0.6942 - val_accuracy: 0.5079 - val_loss: 0.6972\n",
      "Epoch 29/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5172 - loss: 0.6967 - val_accuracy: 0.5070 - val_loss: 0.6970\n",
      "Epoch 30/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5096 - loss: 0.6972 - val_accuracy: 0.5056 - val_loss: 0.6970\n",
      "Epoch 31/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5259 - loss: 0.6929 - val_accuracy: 0.5042 - val_loss: 0.6969\n",
      "Epoch 32/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5234 - loss: 0.6953 - val_accuracy: 0.5056 - val_loss: 0.6969\n",
      "Epoch 33/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5275 - loss: 0.6932 - val_accuracy: 0.5037 - val_loss: 0.6968\n",
      "Epoch 34/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5267 - loss: 0.6935 - val_accuracy: 0.5046 - val_loss: 0.6967\n",
      "Epoch 35/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5230 - loss: 0.6941 - val_accuracy: 0.5051 - val_loss: 0.6966\n",
      "Epoch 36/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5296 - loss: 0.6924 - val_accuracy: 0.5060 - val_loss: 0.6966\n",
      "Epoch 37/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5213 - loss: 0.6937 - val_accuracy: 0.5070 - val_loss: 0.6966\n",
      "Epoch 38/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5241 - loss: 0.6929 - val_accuracy: 0.5074 - val_loss: 0.6966\n",
      "Epoch 39/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5209 - loss: 0.6946 - val_accuracy: 0.5051 - val_loss: 0.6965\n",
      "Epoch 40/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.5217 - loss: 0.6934 - val_accuracy: 0.5051 - val_loss: 0.6965\n",
      "Epoch 41/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5236 - loss: 0.6937 - val_accuracy: 0.5051 - val_loss: 0.6964\n",
      "Epoch 42/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5323 - loss: 0.6918 - val_accuracy: 0.5065 - val_loss: 0.6964\n",
      "Epoch 43/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5326 - loss: 0.6901 - val_accuracy: 0.5056 - val_loss: 0.6964\n",
      "Epoch 44/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5266 - loss: 0.6916 - val_accuracy: 0.5088 - val_loss: 0.6964\n",
      "Epoch 45/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5257 - loss: 0.6925 - val_accuracy: 0.5102 - val_loss: 0.6963\n",
      "Epoch 46/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.5256 - loss: 0.6919 - val_accuracy: 0.5083 - val_loss: 0.6963\n",
      "Epoch 47/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5238 - loss: 0.6922 - val_accuracy: 0.5060 - val_loss: 0.6963\n",
      "Epoch 48/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5246 - loss: 0.6929 - val_accuracy: 0.5070 - val_loss: 0.6962\n",
      "Epoch 49/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5293 - loss: 0.6915 - val_accuracy: 0.5093 - val_loss: 0.6963\n",
      "Epoch 50/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5266 - loss: 0.6924 - val_accuracy: 0.5088 - val_loss: 0.6963\n",
      "Epoch 51/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5344 - loss: 0.6893 - val_accuracy: 0.5088 - val_loss: 0.6963\n",
      "Epoch 52/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5353 - loss: 0.6911 - val_accuracy: 0.5083 - val_loss: 0.6962\n",
      "Epoch 53/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5389 - loss: 0.6890 - val_accuracy: 0.5107 - val_loss: 0.6963\n",
      "Epoch 54/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5313 - loss: 0.6915 - val_accuracy: 0.5111 - val_loss: 0.6963\n",
      "Epoch 55/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5255 - loss: 0.6915 - val_accuracy: 0.5121 - val_loss: 0.6963\n",
      "Epoch 56/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5300 - loss: 0.6910 - val_accuracy: 0.5097 - val_loss: 0.6964\n",
      "Epoch 57/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5272 - loss: 0.6916 - val_accuracy: 0.5088 - val_loss: 0.6963\n",
      "Epoch 58/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5249 - loss: 0.6915 - val_accuracy: 0.5074 - val_loss: 0.6964\n",
      "Epoch 59/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5380 - loss: 0.6892 - val_accuracy: 0.5088 - val_loss: 0.6965\n",
      "Epoch 60/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5349 - loss: 0.6911 - val_accuracy: 0.5093 - val_loss: 0.6964\n",
      "Epoch 61/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5314 - loss: 0.6898 - val_accuracy: 0.5093 - val_loss: 0.6965\n",
      "Epoch 62/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5336 - loss: 0.6904 - val_accuracy: 0.5079 - val_loss: 0.6964\n",
      "Epoch 63/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5315 - loss: 0.6898 - val_accuracy: 0.5093 - val_loss: 0.6964\n",
      "Epoch 64/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5302 - loss: 0.6897 - val_accuracy: 0.5074 - val_loss: 0.6964\n",
      "Epoch 65/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5311 - loss: 0.6909 - val_accuracy: 0.5083 - val_loss: 0.6964\n",
      "Epoch 66/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5394 - loss: 0.6887 - val_accuracy: 0.5065 - val_loss: 0.6964\n",
      "Epoch 67/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5344 - loss: 0.6896 - val_accuracy: 0.5065 - val_loss: 0.6964\n",
      "Epoch 68/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5353 - loss: 0.6895 - val_accuracy: 0.5079 - val_loss: 0.6965\n",
      "Epoch 69/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5375 - loss: 0.6895 - val_accuracy: 0.5088 - val_loss: 0.6964\n",
      "Epoch 70/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5437 - loss: 0.6884 - val_accuracy: 0.5107 - val_loss: 0.6964\n",
      "Epoch 71/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5422 - loss: 0.6883 - val_accuracy: 0.5097 - val_loss: 0.6965\n",
      "Epoch 72/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5349 - loss: 0.6895 - val_accuracy: 0.5107 - val_loss: 0.6965\n",
      "Epoch 73/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5373 - loss: 0.6878 - val_accuracy: 0.5125 - val_loss: 0.6965\n",
      "Epoch 74/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5373 - loss: 0.6911 - val_accuracy: 0.5102 - val_loss: 0.6964\n",
      "Epoch 75/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5322 - loss: 0.6894 - val_accuracy: 0.5097 - val_loss: 0.6965\n",
      "Epoch 76/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5411 - loss: 0.6885 - val_accuracy: 0.5079 - val_loss: 0.6965\n",
      "Epoch 77/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5362 - loss: 0.6895 - val_accuracy: 0.5079 - val_loss: 0.6965\n",
      "Epoch 78/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5379 - loss: 0.6895 - val_accuracy: 0.5083 - val_loss: 0.6966\n",
      "Epoch 79/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5408 - loss: 0.6879 - val_accuracy: 0.5074 - val_loss: 0.6966\n",
      "Epoch 80/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5498 - loss: 0.6861 - val_accuracy: 0.5074 - val_loss: 0.6967\n",
      "Epoch 81/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5353 - loss: 0.6910 - val_accuracy: 0.5130 - val_loss: 0.6967\n",
      "Epoch 82/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5275 - loss: 0.6906 - val_accuracy: 0.5130 - val_loss: 0.6967\n",
      "Epoch 83/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5360 - loss: 0.6893 - val_accuracy: 0.5121 - val_loss: 0.6968\n",
      "Epoch 84/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5431 - loss: 0.6884 - val_accuracy: 0.5121 - val_loss: 0.6968\n",
      "Epoch 85/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5374 - loss: 0.6889 - val_accuracy: 0.5116 - val_loss: 0.6969\n",
      "Epoch 86/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5504 - loss: 0.6861 - val_accuracy: 0.5121 - val_loss: 0.6970\n",
      "Epoch 87/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5398 - loss: 0.6889 - val_accuracy: 0.5116 - val_loss: 0.6970\n",
      "Epoch 88/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5385 - loss: 0.6892 - val_accuracy: 0.5125 - val_loss: 0.6970\n",
      "Epoch 89/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5348 - loss: 0.6891 - val_accuracy: 0.5111 - val_loss: 0.6970\n",
      "Epoch 90/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5430 - loss: 0.6874 - val_accuracy: 0.5125 - val_loss: 0.6971\n",
      "Epoch 91/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5376 - loss: 0.6888 - val_accuracy: 0.5116 - val_loss: 0.6972\n",
      "Epoch 92/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5445 - loss: 0.6879 - val_accuracy: 0.5121 - val_loss: 0.6972\n",
      "Epoch 93/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5370 - loss: 0.6878 - val_accuracy: 0.5121 - val_loss: 0.6973\n",
      "Epoch 94/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5413 - loss: 0.6877 - val_accuracy: 0.5093 - val_loss: 0.6973\n",
      "Epoch 95/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5510 - loss: 0.6874 - val_accuracy: 0.5102 - val_loss: 0.6973\n",
      "Epoch 96/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5451 - loss: 0.6867 - val_accuracy: 0.5111 - val_loss: 0.6974\n",
      "Epoch 97/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5395 - loss: 0.6886 - val_accuracy: 0.5088 - val_loss: 0.6974\n",
      "Epoch 98/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5451 - loss: 0.6877 - val_accuracy: 0.5097 - val_loss: 0.6975\n",
      "Epoch 99/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5452 - loss: 0.6866 - val_accuracy: 0.5079 - val_loss: 0.6975\n",
      "Epoch 100/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5454 - loss: 0.6854 - val_accuracy: 0.5088 - val_loss: 0.6975\n",
      "Epoch 101/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5486 - loss: 0.6858 - val_accuracy: 0.5102 - val_loss: 0.6975\n",
      "Epoch 102/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5485 - loss: 0.6864 - val_accuracy: 0.5097 - val_loss: 0.6977\n",
      "Epoch 103/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5438 - loss: 0.6871 - val_accuracy: 0.5093 - val_loss: 0.6978\n",
      "Epoch 104/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5449 - loss: 0.6871 - val_accuracy: 0.5093 - val_loss: 0.6979\n",
      "Epoch 105/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5501 - loss: 0.6854 - val_accuracy: 0.5088 - val_loss: 0.6980\n",
      "Epoch 106/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5483 - loss: 0.6868 - val_accuracy: 0.5060 - val_loss: 0.6981\n",
      "Epoch 107/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5491 - loss: 0.6865 - val_accuracy: 0.5065 - val_loss: 0.6980\n",
      "Epoch 108/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5421 - loss: 0.6878 - val_accuracy: 0.5088 - val_loss: 0.6980\n",
      "Epoch 109/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5565 - loss: 0.6833 - val_accuracy: 0.5079 - val_loss: 0.6981\n",
      "Epoch 110/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5526 - loss: 0.6856 - val_accuracy: 0.5074 - val_loss: 0.6981\n",
      "Epoch 111/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5459 - loss: 0.6879 - val_accuracy: 0.5079 - val_loss: 0.6982\n",
      "Epoch 112/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5390 - loss: 0.6885 - val_accuracy: 0.5088 - val_loss: 0.6982\n",
      "Epoch 113/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5532 - loss: 0.6859 - val_accuracy: 0.5097 - val_loss: 0.6983\n",
      "Epoch 114/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5476 - loss: 0.6864 - val_accuracy: 0.5107 - val_loss: 0.6983\n",
      "Epoch 115/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5432 - loss: 0.6872 - val_accuracy: 0.5102 - val_loss: 0.6984\n",
      "Epoch 116/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5429 - loss: 0.6864 - val_accuracy: 0.5088 - val_loss: 0.6985\n",
      "Epoch 117/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5559 - loss: 0.6846 - val_accuracy: 0.5088 - val_loss: 0.6985\n",
      "Epoch 118/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5530 - loss: 0.6857 - val_accuracy: 0.5079 - val_loss: 0.6985\n",
      "Epoch 119/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5533 - loss: 0.6834 - val_accuracy: 0.5079 - val_loss: 0.6985\n",
      "Epoch 120/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5469 - loss: 0.6858 - val_accuracy: 0.5097 - val_loss: 0.6985\n",
      "Epoch 121/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5531 - loss: 0.6845 - val_accuracy: 0.5074 - val_loss: 0.6985\n",
      "Epoch 122/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5494 - loss: 0.6839 - val_accuracy: 0.5070 - val_loss: 0.6985\n",
      "Epoch 123/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5555 - loss: 0.6846 - val_accuracy: 0.5065 - val_loss: 0.6985\n",
      "Epoch 124/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5492 - loss: 0.6834 - val_accuracy: 0.5093 - val_loss: 0.6986\n",
      "Epoch 125/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5552 - loss: 0.6847 - val_accuracy: 0.5074 - val_loss: 0.6987\n",
      "Epoch 126/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5450 - loss: 0.6847 - val_accuracy: 0.5097 - val_loss: 0.6988\n",
      "Epoch 127/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5626 - loss: 0.6838 - val_accuracy: 0.5093 - val_loss: 0.6988\n",
      "Epoch 128/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5483 - loss: 0.6870 - val_accuracy: 0.5107 - val_loss: 0.6990\n",
      "Epoch 129/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5507 - loss: 0.6867 - val_accuracy: 0.5093 - val_loss: 0.6989\n",
      "Epoch 130/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5517 - loss: 0.6843 - val_accuracy: 0.5107 - val_loss: 0.6990\n",
      "Epoch 131/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5547 - loss: 0.6846 - val_accuracy: 0.5093 - val_loss: 0.6991\n",
      "Epoch 132/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5512 - loss: 0.6840 - val_accuracy: 0.5107 - val_loss: 0.6992\n",
      "Epoch 133/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5573 - loss: 0.6829 - val_accuracy: 0.5093 - val_loss: 0.6992\n",
      "Epoch 134/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5546 - loss: 0.6843 - val_accuracy: 0.5097 - val_loss: 0.6993\n",
      "Epoch 135/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5539 - loss: 0.6841 - val_accuracy: 0.5116 - val_loss: 0.6994\n",
      "Epoch 136/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5558 - loss: 0.6845 - val_accuracy: 0.5102 - val_loss: 0.6994\n",
      "Epoch 137/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5621 - loss: 0.6822 - val_accuracy: 0.5088 - val_loss: 0.6995\n",
      "Epoch 138/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5498 - loss: 0.6839 - val_accuracy: 0.5093 - val_loss: 0.6996\n",
      "Epoch 139/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5590 - loss: 0.6830 - val_accuracy: 0.5093 - val_loss: 0.6996\n",
      "Epoch 140/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5521 - loss: 0.6829 - val_accuracy: 0.5111 - val_loss: 0.6996\n",
      "Epoch 141/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5611 - loss: 0.6832 - val_accuracy: 0.5135 - val_loss: 0.6997\n",
      "Epoch 142/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5549 - loss: 0.6831 - val_accuracy: 0.5135 - val_loss: 0.6997\n",
      "Epoch 143/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5531 - loss: 0.6840 - val_accuracy: 0.5139 - val_loss: 0.6998\n",
      "Epoch 144/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5640 - loss: 0.6820 - val_accuracy: 0.5158 - val_loss: 0.6999\n",
      "Epoch 145/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5549 - loss: 0.6841 - val_accuracy: 0.5144 - val_loss: 0.6999\n",
      "Epoch 146/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5598 - loss: 0.6838 - val_accuracy: 0.5148 - val_loss: 0.7000\n",
      "Epoch 147/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5612 - loss: 0.6819 - val_accuracy: 0.5153 - val_loss: 0.7002\n",
      "Epoch 148/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5597 - loss: 0.6832 - val_accuracy: 0.5153 - val_loss: 0.7003\n",
      "Epoch 149/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5648 - loss: 0.6811 - val_accuracy: 0.5148 - val_loss: 0.7004\n",
      "Epoch 150/150\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5595 - loss: 0.6832 - val_accuracy: 0.5148 - val_loss: 0.7004\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 618us/step - accuracy: 0.5053 - loss: 0.7009\n",
      "Test Loss: 0.7009295225143433\n",
      "Test Accuracy: 0.5056596994400024\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model_rnn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(600,)),  # input shape matches the combined dimension of premise and hypothesis embeddings\n",
    "    tf.keras.layers.Dense(200, activation='tanh'),\n",
    "    tf.keras.layers.Dense(200, activation='tanh'),\n",
    "    tf.keras.layers.Dense(200, activation='tanh'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(200, activation='tanh'),\n",
    "    tf.keras.layers.Dense(200, activation='tanh'),\n",
    "    tf.keras.layers.Dense(200, activation='tanh'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')  # 2 classes: entailment, contradiction\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_rnn.compile(optimizer='adadelta',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_rnn = model_rnn.fit(X_train_rnn, y_train_rnn, epochs=150, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss_rnn, accuracy_rnn = model_rnn.evaluate(X_test_rnn, y_test_rnn)\n",
    "print(f\"Test Loss: {loss_rnn}\")\n",
    "print(f\"Test Accuracy: {accuracy_rnn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                premise hypothesis  label\n",
      "3126  Tony  Shoes (so Clinton will have Shoes and So...        NaN      1\n",
      "3970                            Saint-Germain-des-Pr??s        NaN      1\n",
      "Empty DataFrame\n",
      "Columns: [premise, hypothesis, label]\n",
      "Index: []\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 894us/step - accuracy: 0.4969 - loss: 0.7046\n",
      "Dev Loss: 0.7026088237762451\n",
      "Dev Accuracy: 0.5034881830215454\n"
     ]
    }
   ],
   "source": [
    "# Test the model on the dev set\n",
    "dev_data_path = \"./data/dev.csv\"\n",
    "dev_df = pd.read_csv(dev_data_path)\n",
    "\n",
    "# Print all rows in the dev dataframe where there is a NaN value\n",
    "print(dev_df[dev_df.isna().any(axis=1)])\n",
    "\n",
    "# Convert NaN values to empty strings\n",
    "dev_df = dev_df.fillna('')\n",
    "# Print all rows in the dev dataframe where there is a NaN value\n",
    "print(dev_df[dev_df.isna().any(axis=1)])\n",
    "\n",
    "# Create sentence embeddings for premise and hypothesis using plain RNN\n",
    "premise_embeddings_dev_rnn = [sentence_embedding_rnn(sentence.lower(), embeddings_index) for sentence in dev_df['premise']]\n",
    "hypothesis_embeddings_dev_rnn = [sentence_embedding_rnn(sentence.lower(), embeddings_index) for sentence in dev_df['hypothesis']]\n",
    "X_dev_rnn = np.hstack((np.array(premise_embeddings_dev_rnn), np.array(hypothesis_embeddings_dev_rnn)))\n",
    "y_dev_rnn = dev_df['label'].values\n",
    "\n",
    "loss_dev_rnn, accuracy_dev_rnn = model_rnn.evaluate(X_dev_rnn, y_dev_rnn)\n",
    "print(f\"Dev Loss: {loss_dev_rnn}\")\n",
    "print(f\"Dev Accuracy: {accuracy_dev_rnn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_embedding_lstm(sentence, embeddings_index, lstm_units=300):\n",
    "    words = sentence.split()\n",
    "    embedding_dim = next(iter(embeddings_index.values())).shape[0]\n",
    "    sentence_embedding = np.zeros(embedding_dim)\n",
    "    for word in words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            sentence_embedding += embedding_vector\n",
    "\n",
    "    # Create LSTM layer\n",
    "    lstm_layer = tf.keras.layers.LSTM(lstm_units)\n",
    "    \n",
    "    # Process sentence embedding through LSTM\n",
    "    sentence_embedding_lstm = lstm_layer(tf.expand_dims([sentence_embedding], axis=0))\n",
    "    \n",
    "    return tf.squeeze(sentence_embedding_lstm, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings...\n",
      "Loaded 400001 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe embeddings\n",
    "embedding_path = \"./input/embeddings/glove.6B/glove.6B.300d.txt\"\n",
    "embeddings_index = load_glove_embeddings(embedding_path)\n",
    "\n",
    "# Create sentence embeddings for premise and hypothesis using LSTM\n",
    "premise_embeddings_lstm = [sentence_embedding_lstm(sentence.lower(), embeddings_index) for sentence in df['premise']]\n",
    "hypothesis_embeddings_lstm = [sentence_embedding_lstm(sentence.lower(), embeddings_index) for sentence in df['hypothesis']]\n",
    "\n",
    "# Combine premise and hypothesis embeddings\n",
    "X_lstm = np.hstack((np.array(premise_embeddings_lstm), np.array(hypothesis_embeddings_lstm)))\n",
    "\n",
    "# Labels\n",
    "y_lstm = df['label'].values\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(X_lstm, y_lstm, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.5000 - loss: 0.7364 - val_accuracy: 0.5186 - val_loss: 0.6995\n",
      "Epoch 2/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.4954 - loss: 0.7360 - val_accuracy: 0.5199 - val_loss: 0.6983\n",
      "Epoch 3/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4993 - loss: 0.7310 - val_accuracy: 0.5209 - val_loss: 0.6973\n",
      "Epoch 4/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.4955 - loss: 0.7286 - val_accuracy: 0.5199 - val_loss: 0.6966\n",
      "Epoch 5/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4929 - loss: 0.7307 - val_accuracy: 0.5167 - val_loss: 0.6962\n",
      "Epoch 6/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.4991 - loss: 0.7260 - val_accuracy: 0.5204 - val_loss: 0.6959\n",
      "Epoch 7/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5019 - loss: 0.7239 - val_accuracy: 0.5107 - val_loss: 0.6956\n",
      "Epoch 8/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5028 - loss: 0.7206 - val_accuracy: 0.5158 - val_loss: 0.6954\n",
      "Epoch 9/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5021 - loss: 0.7186 - val_accuracy: 0.5135 - val_loss: 0.6953\n",
      "Epoch 10/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5001 - loss: 0.7166 - val_accuracy: 0.5102 - val_loss: 0.6951\n",
      "Epoch 11/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5015 - loss: 0.7180 - val_accuracy: 0.5107 - val_loss: 0.6950\n",
      "Epoch 12/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5010 - loss: 0.7174 - val_accuracy: 0.5107 - val_loss: 0.6949\n",
      "Epoch 13/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5040 - loss: 0.7175 - val_accuracy: 0.5107 - val_loss: 0.6949\n",
      "Epoch 14/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5033 - loss: 0.7142 - val_accuracy: 0.5088 - val_loss: 0.6948\n",
      "Epoch 15/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5026 - loss: 0.7162 - val_accuracy: 0.5083 - val_loss: 0.6947\n",
      "Epoch 16/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5029 - loss: 0.7151 - val_accuracy: 0.5102 - val_loss: 0.6947\n",
      "Epoch 17/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5009 - loss: 0.7130 - val_accuracy: 0.5083 - val_loss: 0.6946\n",
      "Epoch 18/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5091 - loss: 0.7120 - val_accuracy: 0.5074 - val_loss: 0.6946\n",
      "Epoch 19/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.4972 - loss: 0.7151 - val_accuracy: 0.5046 - val_loss: 0.6946\n",
      "Epoch 20/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.4980 - loss: 0.7173 - val_accuracy: 0.5014 - val_loss: 0.6946\n",
      "Epoch 21/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5056 - loss: 0.7146 - val_accuracy: 0.5000 - val_loss: 0.6946\n",
      "Epoch 22/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5043 - loss: 0.7102 - val_accuracy: 0.5019 - val_loss: 0.6946\n",
      "Epoch 23/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5015 - loss: 0.7113 - val_accuracy: 0.5032 - val_loss: 0.6945\n",
      "Epoch 24/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5039 - loss: 0.7101 - val_accuracy: 0.5019 - val_loss: 0.6945\n",
      "Epoch 25/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5088 - loss: 0.7108 - val_accuracy: 0.5005 - val_loss: 0.6945\n",
      "Epoch 26/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.4990 - loss: 0.7133 - val_accuracy: 0.5023 - val_loss: 0.6946\n",
      "Epoch 27/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5123 - loss: 0.7059 - val_accuracy: 0.5014 - val_loss: 0.6945\n",
      "Epoch 28/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5089 - loss: 0.7058 - val_accuracy: 0.4986 - val_loss: 0.6945\n",
      "Epoch 29/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4968 - loss: 0.7113 - val_accuracy: 0.4968 - val_loss: 0.6945\n",
      "Epoch 30/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4912 - loss: 0.7130 - val_accuracy: 0.4991 - val_loss: 0.6945\n",
      "Epoch 31/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.4983 - loss: 0.7123 - val_accuracy: 0.4972 - val_loss: 0.6945\n",
      "Epoch 32/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5120 - loss: 0.7073 - val_accuracy: 0.4986 - val_loss: 0.6945\n",
      "Epoch 33/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5118 - loss: 0.7048 - val_accuracy: 0.5005 - val_loss: 0.6945\n",
      "Epoch 34/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5126 - loss: 0.7028 - val_accuracy: 0.5000 - val_loss: 0.6944\n",
      "Epoch 35/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4958 - loss: 0.7088 - val_accuracy: 0.5014 - val_loss: 0.6944\n",
      "Epoch 36/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5020 - loss: 0.7072 - val_accuracy: 0.5019 - val_loss: 0.6944\n",
      "Epoch 37/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5113 - loss: 0.7057 - val_accuracy: 0.5014 - val_loss: 0.6943\n",
      "Epoch 38/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5069 - loss: 0.7030 - val_accuracy: 0.4995 - val_loss: 0.6943\n",
      "Epoch 39/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.4999 - loss: 0.7061 - val_accuracy: 0.5005 - val_loss: 0.6944\n",
      "Epoch 40/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5076 - loss: 0.7032 - val_accuracy: 0.5009 - val_loss: 0.6944\n",
      "Epoch 41/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5068 - loss: 0.7038 - val_accuracy: 0.5019 - val_loss: 0.6943\n",
      "Epoch 42/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5050 - loss: 0.7049 - val_accuracy: 0.5019 - val_loss: 0.6943\n",
      "Epoch 43/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5053 - loss: 0.7061 - val_accuracy: 0.5000 - val_loss: 0.6943\n",
      "Epoch 44/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5073 - loss: 0.7042 - val_accuracy: 0.5032 - val_loss: 0.6943\n",
      "Epoch 45/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5138 - loss: 0.7033 - val_accuracy: 0.5028 - val_loss: 0.6943\n",
      "Epoch 46/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5013 - loss: 0.7055 - val_accuracy: 0.5046 - val_loss: 0.6943\n",
      "Epoch 47/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5074 - loss: 0.7059 - val_accuracy: 0.5065 - val_loss: 0.6942\n",
      "Epoch 48/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5081 - loss: 0.7044 - val_accuracy: 0.5070 - val_loss: 0.6942\n",
      "Epoch 49/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5058 - loss: 0.7031 - val_accuracy: 0.5056 - val_loss: 0.6942\n",
      "Epoch 50/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5077 - loss: 0.7033 - val_accuracy: 0.5060 - val_loss: 0.6942\n",
      "Epoch 51/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5092 - loss: 0.7020 - val_accuracy: 0.5074 - val_loss: 0.6941\n",
      "Epoch 52/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5004 - loss: 0.7049 - val_accuracy: 0.5074 - val_loss: 0.6941\n",
      "Epoch 53/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5056 - loss: 0.7024 - val_accuracy: 0.5042 - val_loss: 0.6942\n",
      "Epoch 54/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5080 - loss: 0.7024 - val_accuracy: 0.5065 - val_loss: 0.6942\n",
      "Epoch 55/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5127 - loss: 0.7009 - val_accuracy: 0.5060 - val_loss: 0.6941\n",
      "Epoch 56/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5091 - loss: 0.7013 - val_accuracy: 0.5065 - val_loss: 0.6941\n",
      "Epoch 57/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5001 - loss: 0.7018 - val_accuracy: 0.5060 - val_loss: 0.6941\n",
      "Epoch 58/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5019 - loss: 0.7007 - val_accuracy: 0.5056 - val_loss: 0.6941\n",
      "Epoch 59/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5075 - loss: 0.7014 - val_accuracy: 0.5032 - val_loss: 0.6942\n",
      "Epoch 60/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5085 - loss: 0.7009 - val_accuracy: 0.5032 - val_loss: 0.6942\n",
      "Epoch 61/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5127 - loss: 0.6985 - val_accuracy: 0.5014 - val_loss: 0.6942\n",
      "Epoch 62/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5086 - loss: 0.7015 - val_accuracy: 0.5028 - val_loss: 0.6943\n",
      "Epoch 63/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5147 - loss: 0.6982 - val_accuracy: 0.5023 - val_loss: 0.6943\n",
      "Epoch 64/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5196 - loss: 0.6986 - val_accuracy: 0.5009 - val_loss: 0.6943\n",
      "Epoch 65/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5136 - loss: 0.6996 - val_accuracy: 0.5009 - val_loss: 0.6943\n",
      "Epoch 66/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5091 - loss: 0.6996 - val_accuracy: 0.4977 - val_loss: 0.6943\n",
      "Epoch 67/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5053 - loss: 0.7016 - val_accuracy: 0.4958 - val_loss: 0.6943\n",
      "Epoch 68/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5036 - loss: 0.7029 - val_accuracy: 0.4949 - val_loss: 0.6943\n",
      "Epoch 69/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5114 - loss: 0.6992 - val_accuracy: 0.4977 - val_loss: 0.6943\n",
      "Epoch 70/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5096 - loss: 0.6999 - val_accuracy: 0.4963 - val_loss: 0.6943\n",
      "Epoch 71/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5108 - loss: 0.6997 - val_accuracy: 0.4972 - val_loss: 0.6943\n",
      "Epoch 72/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5187 - loss: 0.6979 - val_accuracy: 0.4963 - val_loss: 0.6943\n",
      "Epoch 73/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5256 - loss: 0.6963 - val_accuracy: 0.4986 - val_loss: 0.6943\n",
      "Epoch 74/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5087 - loss: 0.7001 - val_accuracy: 0.5023 - val_loss: 0.6942\n",
      "Epoch 75/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5197 - loss: 0.6977 - val_accuracy: 0.5009 - val_loss: 0.6943\n",
      "Epoch 76/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5064 - loss: 0.6991 - val_accuracy: 0.5000 - val_loss: 0.6943\n",
      "Epoch 77/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5074 - loss: 0.6985 - val_accuracy: 0.4981 - val_loss: 0.6943\n",
      "Epoch 78/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5128 - loss: 0.6991 - val_accuracy: 0.5005 - val_loss: 0.6942\n",
      "Epoch 79/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5102 - loss: 0.6979 - val_accuracy: 0.4977 - val_loss: 0.6942\n",
      "Epoch 80/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5046 - loss: 0.7008 - val_accuracy: 0.4991 - val_loss: 0.6943\n",
      "Epoch 81/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5141 - loss: 0.6967 - val_accuracy: 0.4986 - val_loss: 0.6943\n",
      "Epoch 82/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5066 - loss: 0.6983 - val_accuracy: 0.4986 - val_loss: 0.6943\n",
      "Epoch 83/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.4962 - loss: 0.7022 - val_accuracy: 0.4963 - val_loss: 0.6943\n",
      "Epoch 84/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5063 - loss: 0.6988 - val_accuracy: 0.4981 - val_loss: 0.6943\n",
      "Epoch 85/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5114 - loss: 0.6975 - val_accuracy: 0.5005 - val_loss: 0.6944\n",
      "Epoch 86/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5126 - loss: 0.6980 - val_accuracy: 0.4986 - val_loss: 0.6944\n",
      "Epoch 87/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5111 - loss: 0.6983 - val_accuracy: 0.5005 - val_loss: 0.6944\n",
      "Epoch 88/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5186 - loss: 0.6960 - val_accuracy: 0.5037 - val_loss: 0.6944\n",
      "Epoch 89/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5143 - loss: 0.6960 - val_accuracy: 0.5032 - val_loss: 0.6944\n",
      "Epoch 90/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5155 - loss: 0.6977 - val_accuracy: 0.5005 - val_loss: 0.6944\n",
      "Epoch 91/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5141 - loss: 0.6958 - val_accuracy: 0.4986 - val_loss: 0.6944\n",
      "Epoch 92/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5138 - loss: 0.6957 - val_accuracy: 0.4981 - val_loss: 0.6944\n",
      "Epoch 93/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5075 - loss: 0.6971 - val_accuracy: 0.4991 - val_loss: 0.6944\n",
      "Epoch 94/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5113 - loss: 0.6977 - val_accuracy: 0.5009 - val_loss: 0.6944\n",
      "Epoch 95/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5109 - loss: 0.6974 - val_accuracy: 0.5014 - val_loss: 0.6944\n",
      "Epoch 96/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5136 - loss: 0.6975 - val_accuracy: 0.4972 - val_loss: 0.6944\n",
      "Epoch 97/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5170 - loss: 0.6953 - val_accuracy: 0.4991 - val_loss: 0.6944\n",
      "Epoch 98/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5043 - loss: 0.6983 - val_accuracy: 0.4995 - val_loss: 0.6945\n",
      "Epoch 99/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5175 - loss: 0.6953 - val_accuracy: 0.4981 - val_loss: 0.6945\n",
      "Epoch 100/100\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5123 - loss: 0.6979 - val_accuracy: 0.4968 - val_loss: 0.6945\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 672us/step - accuracy: 0.5129 - loss: 0.6942\n",
      "Test Loss: 0.6939526796340942\n",
      "Test Accuracy: 0.5164223313331604\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model_lstm = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(600,)),  # input shape matches the combined dimension of premise and hypothesis embeddings\n",
    "    tf.keras.layers.Dense(200, activation='tanh'),\n",
    "    tf.keras.layers.Dense(200, activation='tanh'),\n",
    "    tf.keras.layers.Dense(200, activation='tanh'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(200, activation='tanh'),\n",
    "    tf.keras.layers.Dense(200, activation='tanh'),\n",
    "    tf.keras.layers.Dense(200, activation='tanh'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(200, activation='tanh'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')  # 2 classes: entailment, contradiction\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_lstm.compile(optimizer='adadelta',\n",
    "                   loss='sparse_categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_lstm = model_lstm.fit(X_train_lstm, y_train_lstm, epochs=100, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss_lstm, accuracy_lstm = model_lstm.evaluate(X_test_lstm, y_test_lstm)\n",
    "print(f\"Test Loss: {loss_lstm}\")\n",
    "print(f\"Test Accuracy: {accuracy_lstm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                premise hypothesis  label\n",
      "3126  Tony  Shoes (so Clinton will have Shoes and So...        NaN      1\n",
      "3970                            Saint-Germain-des-Pr??s        NaN      1\n",
      "Empty DataFrame\n",
      "Columns: [premise, hypothesis, label]\n",
      "Index: []\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 860us/step - accuracy: 0.5018 - loss: 0.6947\n",
      "Dev Loss: 0.6935579776763916\n",
      "Dev Accuracy: 0.5071990489959717\n"
     ]
    }
   ],
   "source": [
    "dev_data_path = \"./data/dev.csv\"\n",
    "dev_df = pd.read_csv(dev_data_path)\n",
    "\n",
    "# Print all rows in the dev dataframe where there is a NaN value\n",
    "print(dev_df[dev_df.isna().any(axis=1)])\n",
    "\n",
    "# Convert NaN values to empty strings\n",
    "dev_df = dev_df.fillna('')\n",
    "# Print all rows in the dev dataframe where there is a NaN value\n",
    "print(dev_df[dev_df.isna().any(axis=1)])\n",
    "\n",
    "# Test the model on the dev set\n",
    "premise_embeddings_dev_lstm = [sentence_embedding_lstm(sentence.lower(), embeddings_index) for sentence in dev_df['premise']]\n",
    "hypothesis_embeddings_dev_lstm = [sentence_embedding_lstm(sentence.lower(), embeddings_index) for sentence in dev_df['hypothesis']]\n",
    "X_dev_lstm = np.hstack((np.array(premise_embeddings_dev_lstm), np.array(hypothesis_embeddings_dev_lstm)))\n",
    "y_dev_lstm = dev_df['label'].values\n",
    "\n",
    "loss_dev_lstm, accuracy_dev_lstm = model_lstm.evaluate(X_dev_lstm, y_dev_lstm)\n",
    "print(f\"Dev Loss: {loss_dev_lstm}\")\n",
    "print(f\"Dev Accuracy: {accuracy_dev_lstm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Prediction: 1\n"
     ]
    }
   ],
   "source": [
    "test_premise_sentence = \"A person on a horse jumps over a broken down airplane.\"\n",
    "test_hypothesis_sentence = \"A person is training his horse for a competition.\"\n",
    "test_premise_embedding = sentence_embedding(test_premise_sentence.lower(), embeddings_index)\n",
    "test_hypothesis_embedding = sentence_embedding(test_hypothesis_sentence.lower(), embeddings_index)\n",
    "X_test = np.hstack((test_premise_embedding, test_hypothesis_embedding))\n",
    "X_test = np.expand_dims(X_test, axis=0)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_label = np.argmax(y_pred, axis=1)\n",
    "print(f\"Prediction: {y_pred_label[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
